{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: MNIST hand-written digits dataset\n",
    "\n",
    "##### License: Apache 2.0\n",
    "\n",
    "\n",
    "This notebook shows how to use *giotto-tda* to generate features for classifying digits. We first show how to build a few topological features and present a pipeline extracting a very large amount of features for classification.\n",
    "\n",
    "The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "The first step consists in importing relevant *gtda* components and other useful libraries or modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from gtda.images import Binarizer, Inverter, ImageToPointCloud, HeightFiltration, DilationFiltration, RadialFiltration, ErosionFiltration, SignedDistanceFiltration\n",
    "from gtda.homology import CubicalPersistence\n",
    "from gtda.diagrams import ForgetDimension, Amplitude, Scaler, PersistenceEntropy, BettiCurve, PersistenceLandscape, HeatKernel, Silhouette\n",
    "from gtda.plotting import plot_heatmap, plot_betti_curves, plot_diagram\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# This would be quite nice but is available with sklearn >= 0.23\n",
    "# from sklearn import set_config\n",
    "#set_config(display='diagram') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "(X, y) = fetch_openml(data_id=554, return_X_y=True)\n",
    "X = X.reshape((-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# For a full-blown example, you can set \n",
    "# n_train, n_test = 60000, 10000\n",
    "n_train, n_test = 60, 10\n",
    "\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:n_train+n_test]\n",
    "y_test = y[n_train:n_train+n_test]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples of the input data\n",
    "We choose the first 20 samples from the training set and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=0.4)\n",
    "X_train_binarized = binarizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE TO UMBE: plot_heatmap does not support binary arrays\n",
    "plot_heatmap(X_train_binarized[1]*1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting the boolean images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverter = Inverter(n_jobs=4)\n",
    "X_train_inverted = inverter.fit_transform(X_train_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(X_train_inverted[1]*1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a boolean image filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 28\n",
    "\n",
    "erosion_filtration = ErosionFiltration(n_iterations=n_iterations, n_jobs=4)\n",
    "X_train_filtered = erosion_filtration.fit_transform(X_train_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(X_train_filtered[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting persistence diagrams out of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubical_complex = CubicalPersistence(n_jobs=1)\n",
    "X_train_cubical = cubical_complex.fit_transform(X_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(X_train_cubical[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the betti curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betti = BettiCurve(n_bins=36, n_jobs=1)\n",
    "X_train_betti = betti.fit_transform(X_train_cubical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betti.plot(X_train_betti, sample=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the heat kernel of stacked diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_stacker = ForgetDimension()\n",
    "X_train_stacked = diagram_stacker.fit_transform(X_train_cubical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = HeatKernel(sigma=3., n_bins=36, n_jobs=1)\n",
    "X_train_heat = heat.fit_transform(X_train_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(X_train_heat[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_heat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling the diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {'metric': 'bottleneck', 'metric_params': {}}\n",
    "\n",
    "diagram_scaler = Scaler(**metric)\n",
    "diagram_scaler.fit(X_train_cubical)\n",
    "X_train_scaled = diagram_scaler.transform(X_train_cubical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_scaler.plot(X_train_scaled, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a pipeline to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('binarizer', Binarizer(threshold=0.4)),\n",
    "    ('filtration', SignedDistanceFiltration(n_iterations=28)),\n",
    "    ('diagram', CubicalPersistence(n_jobs=1)),\n",
    "    ('amplitude', Amplitude(metric='wasserstein', metric_params={'p': 2}, n_jobs=1))\n",
    "    ]\n",
    "\n",
    "pipeline_signed_distance = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pipeline_distance = pipeline_signed_distance.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining features from different filtrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_list = [ np.array([0, 1]), np.array([0, -1]), np.array([1, 0]), np.array([-1, 0]) ]\n",
    "\n",
    "filtration_list = [HeightFiltration(direction=direction) \n",
    "                    for direction in direction_list]\n",
    "\n",
    "steps_list = [ [\n",
    "    ('binarizer', Binarizer(threshold=0.4)),\n",
    "    ('filtration', filtration),\n",
    "    ('diagram', CubicalPersistence()),\n",
    "    ('amplitude', Amplitude(metric='heat', metric_params={'p': 2}))]\n",
    "    for filtration in filtration_list ]\n",
    "\n",
    "pipeline_list = [ (str(direction_list[i]), Pipeline(steps_list[i])) for i in range(len(steps_list))]\n",
    "feature_union_filtrations = FeatureUnion(pipeline_list, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_union_filtrations.fit(X_train[:20])\n",
    "X_train_filtrations = feature_union_filtrations.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving a full-scale TDA feature extraction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go full-scale and extract a large number of features. Careful, some of them will be highly correlated, so it would be good to use a feature selection algorithm to reduce their number before passing them to a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_list = [ [1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1], [0, -1], [1, -1] ] \n",
    "center_list = [ [13, 6], [6, 13], [13, 13], [20, 13], [13, 20], [6, 6], [6, 20], [20, 6], [20, 20] ]\n",
    "n_iterations_erosion_list = [6, 10]\n",
    "n_iterations_dilation_list = [6, 10]\n",
    "n_iterations_signed_list = [6, 10]\n",
    "n_neighbors_list = [2, 4]\n",
    "\n",
    "# Creating a list of all filtration transformer, we will be applying\n",
    "filtration_list =  [HeightFiltration(direction=np.array(direction)) \n",
    "                    for direction in direction_list] \\\n",
    "                 + [RadialFiltration(center=np.array(center)) \n",
    "                    for center in center_list] \\\n",
    "                 + [ErosionFiltration(n_iterations=n_iterations) \n",
    "                    for n_iterations in n_iterations_erosion_list] \\\n",
    "                 + [DilationFiltration(n_iterations=n_iterations) \n",
    "                    for n_iterations in n_iterations_dilation_list] \\\n",
    "                 + [SignedDistanceFiltration(n_iterations=n_iterations) \n",
    "                    for n_iterations in n_iterations_signed_list] \\\n",
    "                 + ['passthrough']\n",
    "\n",
    "# Creating the diagram generation pipeline\n",
    "diagram_steps = [[Binarizer(threshold=0.4), \n",
    "                  filtration, \n",
    "                  CubicalPersistence(homology_dimensions=[0, 1]), \n",
    "                  Scaler(metric='bottleneck')] \n",
    "                  for filtration in filtration_list]\n",
    "\n",
    "# Listing all metrics we want to use to extract diagram amplitudes\n",
    "metric_list = [ \n",
    "   {'metric': 'bottleneck', 'metric_params': {}},\n",
    "   {'metric': 'wasserstein', 'metric_params': {'p': 1}},\n",
    "   {'metric': 'wasserstein', 'metric_params': {'p': 2}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 1, 'n_layers': 1, 'n_bins': 100}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 1, 'n_layers': 2, 'n_bins': 100}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 2, 'n_layers': 1, 'n_bins': 100}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 2, 'n_layers': 2, 'n_bins': 100}},\n",
    "   {'metric': 'betti', 'metric_params': {'p': 1, 'n_bins': 100}},\n",
    "   {'metric': 'betti', 'metric_params': {'p': 2, 'n_bins': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 1, 'sigma': 1.6, 'n_bins': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 1, 'sigma': 3.2, 'n_bins': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 2, 'sigma': 1.6, 'n_bins': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 2, 'sigma': 3.2, 'n_bins': 100}}\n",
    "]\n",
    "\n",
    "#\n",
    "feature_union = make_union(*[PersistenceEntropy()] + [Amplitude(**metric, order=None) \n",
    "                                                      for metric in metric_list])\n",
    "\n",
    "tda_union = make_union(*[make_pipeline(*diagram_step, feature_union)\n",
    "                         for diagram_step in diagram_steps], n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tda = tda_union.fit_transform(X_train)\n",
    "X_train_tda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated 672 topological features per image! Now, those features were not chosen properly and some of them are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a hyperparameter search to find the best one! Let's do it for a simple pipeline that uses HeightFiltration and let's find the best direction for a classification problem using a RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_pipeline = Pipeline([\n",
    "    ('binarizer', Binarizer(threshold=0.4)),\n",
    "    ('filtration', HeightFiltration()),\n",
    "    ('diagram', CubicalPersistence()),\n",
    "    ('feature', PersistenceEntropy()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tune features hyper parameters and classifier hyper parameters together in a single hyper parameters grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_list = [ [1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1], [0, -1], [1, -1] ] \n",
    "homology_dimensions_list = [ [0], [1] ]\n",
    "n_estimators_list = [ 500, 1000, 2000 ]\n",
    "\n",
    "param_grid = {\n",
    "    'filtration__direction': [np.array(direction) for direction in direction_list],\n",
    "    'diagram__homology_dimensions' : [homology_dimensions for homology_dimensions in homology_dimensions_list],\n",
    "    'classifier__n_estimators': [n_estimators for n_estimators in n_estimators_list]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=height_pipeline, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on validation set:\")\n",
    "print()\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on validation set:\")\n",
    "print()\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a full report on the grid search result! Even on this very small train set, HeightFiltration with direction [1, 0] in dimension 0 (connected components) provides the most promising feature! Can you interpret why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
